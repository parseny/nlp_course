{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parseny/nlp_course/blob/2023/week05_transfer/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlu-s2k9D1Ba"
      },
      "source": [
        "### Homework 5: Question search engine\n",
        "\n",
        "Remeber week01 where you used GloVe embeddings to find related questions? That was.. cute, but far from state of the art. It's time to really solve this task using context-aware embeddings.\n",
        "\n",
        "__Warning:__ this task assumes you have seen `seminar.ipynb`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HYffoHiI8du5",
        "outputId": "4c13e05b-2acb-4f6d-97d4-c6cb4a00f8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.13)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade transformers datasets accelerate deepspeed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfSHyQlT-fVF"
      },
      "source": [
        "### Load data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2_wgtrx8e6C",
        "outputId": "fa8eae3b-2159-4190-968b-78c9f5bc6f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sample[0]: {'text1': 'How is the life of a math student? Could you describe your own experiences?', 'text2': 'Which level of prepration is enough for the exam jlpt5?', 'label': 0, 'idx': 0, 'label_text': 'not duplicate'}\n",
            "Sample[3]: {'text1': 'What can one do after MBBS?', 'text2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'label_text': 'duplicate'}\n"
          ]
        }
      ],
      "source": [
        "qqp = datasets.load_dataset('SetFit/qqp')\n",
        "print('\\n')\n",
        "print(\"Sample[0]:\", qqp['train'][0])\n",
        "print(\"Sample[3]:\", qqp['train'][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pStlWcvD8rdk"
      },
      "outputs": [],
      "source": [
        "model_name = \"gchhablani/bert-base-cased-finetuned-qqp\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM3ZujeZ-Z7E"
      },
      "source": [
        "### Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qtkllSPG9bTL"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 128\n",
        "def preprocess_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples['text1'], examples['text2'],\n",
        "        padding='max_length', max_length=MAX_LENGTH, truncation=True\n",
        "    )\n",
        "    result['label'] = examples['label']\n",
        "    return result\n",
        "\n",
        "qqp_preprocessed = qqp.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMcFN59_Ll2",
        "outputId": "98836b89-5d53-458c-e987-86492b36797d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1731, 1110, 1103, 1297, 1104, 170, 12523, 2377, 136, 7426, 1128, 5594, 1240, 1319, 5758, 136,  ...\n"
          ]
        }
      ],
      "source": [
        "print(repr(qqp_preprocessed['train'][0]['input_ids'])[:100], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyQ1ZbzGAUF2"
      },
      "source": [
        "### Task 1: evaluation (1 point)\n",
        "\n",
        "We randomly chose a model trained on QQP - but is it any good?\n",
        "\n",
        "One way to measure this is with validation accuracy - which is what you will implement next.\n",
        "\n",
        "Here's the interface to help you do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M5ueSoieAbBg"
      },
      "outputs": [],
      "source": [
        "val_set = qqp_preprocessed['validation']\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=1650, shuffle=False, collate_fn=transformers.default_data_collator, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsPwXXx-At-i"
      },
      "outputs": [],
      "source": [
        "for batch in val_loader:\n",
        "     break  # here be your training code\n",
        "print(\"Sample batch:\", batch)\n",
        "\n",
        "with torch.no_grad():\n",
        "  predicted = model(\n",
        "      input_ids=batch['input_ids'],\n",
        "      attention_mask=batch['attention_mask'],\n",
        "      token_type_ids=batch['token_type_ids']\n",
        "  )\n",
        "\n",
        "print('\\nPrediction (probs):', torch.softmax(predicted.logits, dim=1).data.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoxHzxn0DQqO"
      },
      "source": [
        "__Your task__ is to measure the validation accuracy of your model.\n",
        "Doing so naively may take several hours. Please make sure you use the following optimizations:\n",
        "\n",
        "- run the model on GPU with no_grad\n",
        "- using batch size larger than 1\n",
        "- use optimize data loader with num_workers > 1\n",
        "- (optional) use [mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "QyKOHHmH-Vj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k5EK7-KA5F2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "accuracy = []\n",
        "for batch in val_loader:\n",
        "  with torch.no_grad():\n",
        "    predicted = model(\n",
        "      input_ids=batch['input_ids'],\n",
        "      attention_mask=batch['attention_mask'],\n",
        "      token_type_ids=batch['token_type_ids']\n",
        "    )\n",
        "    logits = predicted.logits\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct_preds = (preds == batch['labels']).float().sum()\n",
        "    m = correct_preds / len(preds)\n",
        "    accuracy.append(m)\n",
        "\n",
        "accuracy = torch.mean(torch.tensor(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 0.9 < accuracy < 0.91"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R2z_-FZU3qy",
        "outputId": "bdccca39-2fc9-462c-b3a7-30b5fc9e86ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9083848627256987"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KONQ1E0J-y6B"
      },
      "source": [
        "### Task 2: train the model (4 points)\n",
        "\n",
        "For this task, you have two options:\n",
        "\n",
        "__Option A:__ fine-tune your own model. You are free to choose any model __except for the original BERT.__ We recommend [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base). Better yet, choose the best model based on public benchmarks (e.g. [GLUE](https://gluebenchmark.com/)).\n",
        "\n",
        "You can write the training code manually or use transformers.Trainer (see [this example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification)). Please make sure that your model's accuracy is at least __comparable__ with the above example for BERT.\n",
        "\n",
        "\n",
        "__Option B:__ compare at least 3 pre-finetuned models (in addition to the above BERT model). For each model, report (1) its accuracy, (2) its speed, measured in samples per second in your hardware setup and (3) its size in megabytes. Please take care to compare models in equal setting, e.g. same CPU / GPU. Compile your results into a table and write a short (~half-page on top of a table) report, summarizing your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ZkZTkl_yMU"
      },
      "outputs": [],
      "source": [
        "<A whole lot of your code goes here>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpC8Uf82Lqxd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQD0IV44LrSs"
      },
      "source": [
        "### Task 3: try the full pipeline (1 point)\n",
        "\n",
        "Finally, it is time to use your model to find duplicate questions.\n",
        "Please implement a function that takes a question and finds top-5 potential duplicates in the training set. For now, it is fine if your function is slow, as long as it yields correct results.\n",
        "\n",
        "Showcase how your function works with at least 5 examples."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLSjmsKaUyQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Bonus:__ for bonus points, try to find a way to run the function faster than just passing over all questions in a loop. For isntance, you can form a short-list of potential candidates using a cheaper method, and then run your tranformer on that short list. If you opted for this solution, please keep both the original implementation and the optimized one - and explain briefly what is the difference there."
      ],
      "metadata": {
        "id": "h86vREj7U02k"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}